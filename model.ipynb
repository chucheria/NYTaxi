{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "\n",
    "The client is launching a new ride sharing program in New York similar to Uber or Lyft. At the end of each trip they want their app to suggest a tip amount to the rider. The company has not acquired any of their own data yet, so they have tasked you with producing a model based off of the taxi data. This model should predict the likely tip amount for a trip based on the other trip attributes. You can assume that the ride sharing company can provide data that has the same attributes as the taxi data for each trip.\n",
    "\n",
    "In building the model consider the following requirements:\n",
    "\n",
    "- The model should be built from the taxi dataset. You can supplement the taxi data with external datasets, but this is not a requirement.\n",
    "- Document your choice of model / algorithm, discussing why you chose it over alternatives.\n",
    "    - Document how you assess your models performance.\n",
    "    - Discuss any limitations or caveats of the model which might be an issue in implementing it.\n",
    "    - Discuss how you might improve your model going forward.\n",
    "    - Discuss how you might turn this model in to an API the company can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "df = dd.read_csv('data/data_new-vars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
       "       'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag',\n",
       "       'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra',\n",
       "       'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge',\n",
       "       'total_amount', 'PUdate', 'PUhour', 'DOdate', 'DOhour',\n",
       "       'trip_duration'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RatecodeID         int64\n",
       "trip_distance    float64\n",
       "PULocationID       int64\n",
       "DOLocationID       int64\n",
       "fare_amount      float64\n",
       "extra            float64\n",
       "tolls_amount     float64\n",
       "total_amount     float64\n",
       "PUhour             int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "train, test = df.random_split([0.80, 0.20], random_state=42)\n",
    "y_train = train.tip_amount\n",
    "y_test = test.tip_amount\n",
    "X_train = train[['RatecodeID', 'trip_distance',\n",
    "       'PULocationID', 'DOLocationID', 'fare_amount', 'extra', 'tolls_amount',\n",
    "       'total_amount', 'PUhour']]\n",
    "X_test = test[['RatecodeID', 'trip_distance',\n",
    "       'PULocationID', 'DOLocationID', 'fare_amount', 'extra', 'tolls_amount',\n",
    "       'total_amount', 'PUhour']]\n",
    "X_test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import GridSearchCV\n",
    "from dask_ml.xgboost import XGBRegressor\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBRegressor()\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for XGBoost:  1.5988745755291442\n"
     ]
    }
   ],
   "source": [
    "from dask_ml.metrics import mean_squared_error\n",
    "print('RMSE for XGBoost: ', mean_squared_error(y_test, pred, multioutput='raw_values').compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose a XGBoost model because it performs significally well as a regresor, there's no need for feature normalization, it reduces overfitting, and trains relatively fast. I didn't make any feature engineering which is one of the reasons this model leaves room (much room) for improvement.\n",
    "\n",
    "Having said that, neural networks (LSTM) usually perform better but they are slower to train and the model engineering needs more work. \n",
    "\n",
    "Next steps to improve this model:\n",
    "    \n",
    "    - Do feature engineeering.\n",
    "    - Move from sklearn to TensorFlow or Coffee to customize the model and increase accuracy.\n",
    "    - Move to a parallel model.\n",
    "    - Change the model in case the performance doesn't improve much as said before.\n",
    "    \n",
    "To make a API we would have to use a stream to calculate predictions based on the model used for input. Basically, each time data is upload the model would predict and return those predictions. The model should be trained once a day/week/etc to maintain accuracy depending on how often new data is entered in the system.\n",
    "This workflow has a component of data cleaning and wrangling for the model to be able to interpret it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
